{"cells":[{"metadata":{"_uuid":"20c011dd401be7b6448c43f965e5d0bf548c53b9","_cell_guid":"e084e610-8128-4769-ab64-6aa194044892"},"cell_type":"markdown","source":"# Tongue Twister Dataset\nOne of my personal favourite aspects of linguistics are Tongue twisters. Thus, when I was looking on the net and found no datasets for this, I created one.        \nWhile I currently just have ~600 tongue twisters, I believe an increase in data is easily possible for this topic due to the excellent community we're a part of.         \nTherefore this is sort of an Open Source dataset that you can contribute to! [Here is the GitHub](https://github.com/djthegr8/tongue-twisters) if you want to Contribute\n\n\n### Special Thanks \nInstead of starting a notebook with a word level LSTM from scratch, I have reused this Beginner's Guide to LSTMs from Shivam Bansal here, so thanks to them.\n\n\n\n## Import the libraries\n\nAs the first step, we need to import the required libraries:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# keras module for building LSTM \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.utils as ku \n# String for getting punctuation\nfrom string import punctuation\n# set seeds for reproducability\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(2)\nseed(1)\n# I like to generally import tensorflow, just in case.\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"## Load the dataset\n\nLoad the dataset of tongue twisters. "},{"metadata":{"_uuid":"87836e3adbe046dd0db62013491ba62bae93b6be","_cell_guid":"b8ef1429-ff19-4a6c-92d7-af8cc61c55f7","trusted":true},"cell_type":"code","source":"all_headlines = open('../dataset/Text Formats/database.txt', 'rb').read().decode(encoding='utf-8').split('|')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fda5d4868631d3618d4d9a9a863541b2faf121c0","_cell_guid":"9dbd8bc9-fb61-43b9-b0c4-98bd7f3f8150"},"cell_type":"markdown","source":"## Dataset preparation\n\n### Dataset cleaning \n\nIn dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words. "},{"metadata":{"_uuid":"2a07365a27a7ba2f92fc9ba4d05d8e6254a68d8c","_cell_guid":"b8bf84ed-da11-4f89-a584-9dceea677420","trusted":true},"cell_type":"code","source":"def clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n\ncorpus = [clean_text(x) for x in all_headlines]\ncorpus[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Don't worry, even I can't pronounce all of them without rolling my tongue off"},{"metadata":{"_uuid":"6fd11859fd71aa5c7ce10bdbbd31c8eb6d1b3118","_cell_guid":"9d83cc08-19ba-4b00-9ca6-dcf5ff39c8af"},"cell_type":"markdown","source":"### Generating Sequence of N-gram Tokens\n\nLanguage modelling requires a sequence input data, as given a sequence (of words/tokens) the aim is the predict next word/token.  \n\nThe next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Pythonâ€™s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens. \n"},{"metadata":{"_uuid":"9129a8b773feb72eff91aa0025157a173d10c625","_cell_guid":"896543c9-7944-4748-b8ef-ef8cbc2a84f0","trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(corpus)\ninp_sequences[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca588b414e70e21bebcead960f6632805d37dd8c","_cell_guid":"73254551-40bd-45b1-a7a5-88fe4cbe0b20","trusted":true},"cell_type":"code","source":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b8a64b96011f427c48d5b0819e3e74af604ce43","_cell_guid":"8b5d80ff-54a8-4380-8a3c-149be880551d"},"cell_type":"markdown","source":"Perfect, now we can obtain the input vector X and the label vector Y which can be used for the training purposes. Recent experiments have shown that recurrent neural networks have shown a good performance in sequence to sequence learning and text data applications. Lets look at them in brief.\n\n## LSTMs for Text Generation\n\n![](http://www.shivambansal.com/blog/text-lstm/2.png)\n\nLSTM's major layers are below..\n\n1. Input Layer : Takes the sequence of words as input\n2. LSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\n3. Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting. (Optional Layer)\n4. Output Layer : Computes the probability of the best possible next word as output\n\nWe will run this model for total 50 epochs but it can be experimented further."},{"metadata":{"_uuid":"76ef6d9352002d333a7c75e8aed7ce996015f527","_cell_guid":"60d6721e-e40e-4f2b-8f63-c06459d68f26","trusted":true},"cell_type":"code","source":"def create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    # Add Input Embedding Layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    # Add Hidden Layer 1 - LSTM Layer\n    model.add(LSTM(50))\n    model.add(Dropout(0.1))\n    \n    # Add Output Layer\n    model.add(Dense(total_words, activation='softmax'))\n    opt = tf.keras.optimizers.Adam(lr=0.005)\n    model.compile(loss='categorical_crossentropy', optimizer=opt)\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0b16b471969dbb831cb0024e303341e11b63de4","_cell_guid":"1826aa1a-cb77-4379-a69d-e9b180945dce"},"cell_type":"markdown","source":"Lets train our model now. 50 epochs is actually really overfittable, as you will see."},{"metadata":{"_uuid":"156f3303b8120cc6932e6db985cbea4a7ceb08bf","_cell_guid":"07d5cf03-d171-4993-9f8b-18446649ecb0","trusted":true},"cell_type":"code","source":"history = model.fit(predictors, label, epochs=50, verbose=True, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I don't know if i'll be here when it completes, so here i go\nmodel.save('tongue_twister.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"448bf43b123060dfe4e27cb9f12889e4fe0ed2a7","_cell_guid":"61e99cfe-7395-4d61-8d1a-8539103d3db5"},"cell_type":"markdown","source":"## 5. Generating the text \n\nGreat, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.\n"},{"metadata":{"_uuid":"e71e56543b7065f115a05e3fd062262b3b94ad46","trusted":true},"cell_type":"code","source":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c49bf4ea0e54f3145149e164e243d897f545b84c","_cell_guid":"ea0bddb6-acc6-4592-a2e0-ffc4129a582f"},"cell_type":"markdown","source":"## 6. Some Results\nWell, as you can see, the model thinks I'm a dog green ðŸ¤£       \nJokes apart, as you can see the outputs aren't too (or in any way) sensible, but ***they're not supposed to!*** \nWe've trained them to make weirdly sounding sentences, and can't disagree with `Donald Duck Fish Rutter Watch Fried`"},{"metadata":{"_uuid":"a21548224c9e661a29e3d369e348aada0599bdc9","_cell_guid":"e38dd280-093b-4091-b82b-9aa90045b107","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print (generate_text(\"Dweep Joshipura is\", 3, model, max_sequence_len))\nprint (generate_text(\"donald duck\", 4, model, max_sequence_len))\nprint (generate_text(\"india and china\", 4, model, max_sequence_len))\nprint (generate_text(\"new york\", 4, model, max_sequence_len))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"279f2e20c482b40d707413d0b1842f179a0d3d7b","_cell_guid":"b2cfe563-974a-4e05-ad60-233d409d3de1"},"cell_type":"markdown","source":"## Improvement Ideas \n\nAs we can see, the model has produced the output which looks Really bad. However, this is due to lack of data (only ~600 examples).                           \nThe results can be improved with following points:\n- Adding more data\n- Fine Tuning the network architecture\n- Fine Tuning the network parameters\n\nThanks for going through the notebook!                           \nI hope this dataset will be helpful to you!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
